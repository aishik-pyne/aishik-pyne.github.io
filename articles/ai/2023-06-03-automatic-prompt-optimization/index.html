<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/brands.min.css><link rel=stylesheet type=text/css href=https://aishikpyne.com/css/style.css><script src=https://aishikpyne.com/js/darkmode.js></script><title>Aishik Pyne</title><script type=text/x-mathjax-config>
        MathJax.Hub.Config({
          TeX: {
              equationNumbers: {
                  autoNumber: "AMS"
              }
          },
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true,
          }
        });
        MathJax.Hub.Register.MessageHook("Math Processing Error", function (message) {
            alert("Math Processing Error: " + message[1]);
        });
        MathJax.Hub.Register.MessageHook("TeX Jax - parse error", function (message) {
            alert("Math Processing Error: " + message[1]);
      });
      </script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="bg-zinc-50 dark:bg-black h-screen flex flex-col"><div class="fixed inset-0 flex justify-center tablet:px-8"><div class="flex w-full max-w-7xl desktop:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><nav class="fixed w-full mx-auto z-20 top-0 tablet:px-8"><div class="mx-auto max-w-7xl w-full desktop:px-8 mb-10"><div class="flex flex-wrap items-center p-4 tablet:px-4 bg-white dark:bg-zinc-900 gap-4"><div class="flex initial order-first"><a href=https://aishikpyne.com/ class="flex items-center"><span class="self-center whitespace-nowrap font-signature text-lg font-semibold dark:text-white">Aishik Pyne</span></a></div><div id=navbar-search class="hidden tablet:block mx-auto justify-center tablet:order-2 order-4 grow"><div class="flex justify-center"><ul class="flex items-center tablet:w-auto w-min justify-center tablet:flex-wrap rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/about>About</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/projects>Projects</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/articles>Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/photography>Photography</a></li></ul></div></div><div class="flex grow tablet:grow-0 items-end justify-end order-3"><button id=theme-toggle type=button class="mx-2 rounded-lg p-2.5 text-sm text-zinc-500 hover:bg-zinc-100 focus:outline-none focus:ring-4 focus:ring-zinc-200 dark:text-zinc-400 dark:hover:bg-zinc-700 dark:focus:ring-zinc-700"><svg id="theme-toggle-dark-icon" class="hidden h-5 w-5" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden h-5 w-5" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button>
<button data-collapse-toggle=navbar-search type=button class="inline-flex items-center rounded-lg p-2 text-sm text-gray-500 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600 tablet:hidden" aria-controls=navbar-search aria-expanded=false>
<span class=sr-only>Open menu</span><svg class="h-6 w-6" aria-hidden="true" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"/></svg></button></div></div></div></nav><div class="relative flex flex-col w-full mx-auto pt-16"><div class="max-w-prose m-6 mx-auto px-4 tablet:px-8"><header class="not-format mb-4 lg:mb-6 max-w-prose"><h1 class="mb-2 text-3xl font-extrabold leading-tight text-gray-900 dark:text-white tablet:text-3xl">Automatic Prompt Optimization (ADO) with "Gradient Decent" and Beam Search</h1><p class="text-base font-light text-gray-500 dark:text-gray-400">Jan 1, 0001 · Aishik Pyne · 7 min</p></header><div class="mx-auto mb-4 max-w-prose rounded-lg border border-zinc-200 bg-white px-4 py-2 shadow dark:border-zinc-700 dark:bg-zinc-800 dark:text-white" id=accordion-collapse data-accordion=collapse><div id=accordion-collapse-heading-1 class=m0><button type=button class="flex w-full cursor-zoom-in items-center justify-between bg-inherit text-left font-medium text-zinc-900 dark:bg-inherit dark:text-zinc-200" data-accordion-target=#accordion-collapse-body-1 aria-expanded=false aria-controls=accordion-collapse-body-1>
<span class=text-bold>Table of Content</span><svg class="h-6 w-6 shrink-0" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414.0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414.0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button></div><div id=accordion-collapse-body-1 class="prose prose-sm prose-a:no-underline hidden leading-tight dark:prose-invert" aria-labelledby=accordion-collapse-heading-1><nav id=TableOfContents><ul><li><a href=#gradient-descent-with-prompts>Gradient Descent with Prompts</a></li><li><a href=#beam-search-over-prompts>Beam search over prompts</a><ul><li><a href=#expansion-step>Expansion Step</a></li><li><a href=#selection-step>Selection Step</a></li></ul></li><li><a href=#experiments>Experiments</a></li></ul><ul><li><a href=#citation>Citation</a></li><li><a href=#references>References</a></li></ul></nav></div></div><article class="prose prose-zinc max-w-[100ch] leading-relaxed mx-auto dark:prose-invert
prose-img:mx-auto prose-img:rounded-lg
prose-figcaption:-mt-6 prose-figcaption:text-center"><p>Given you have a task $x$, and an Large Language Model $LLM_p(x)$, what is a best prompt $p$ which produces the desired solution $y$?</p><p><a href=http://arxiv.org/abs/2305.03495>“Automatic Prompt Optimization with ‘Gradient Descent’ and Beam Search.”</a> tries to answer this question, by optimizing the discrete prompt $p$, with non-paramteric &ldquo;gradient decent&rdquo; and <a href="https://en.wikipedia.org/wiki/Beam_search#:~:text=In%20computer%20science%2C%20beam%20search,that%20reduces%20its%20memory%20requirements">beam search</a>.</p><h1 id=problem-formulation class="group hover:underline hover:underline-offset-6">Problem Formulation <a href=#problem-formulation class="opacity-0 group-hover:opacity-100">#</a></h1><p>The proposed Automatic Prompt Optimization (APO) framework assumes access to:</p><ol><li>An <strong>initial prompt</strong> $p_0$ which is usually naively handcrafted to solve the task $T(x) \rightarrow y$, where the the prompt $p_0$, query $x$ and ground-truth output $y$ all belong to the space of coherent natural language $\mathcal{L}$</li><li>An <strong><a href=https://en.wikipedia.org/wiki/Independent_and_identically_distributed_random_variables>i.i.d</a> dataset</strong>, $D_{tr} = {(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)}$ corresponding to the task $T$.</li><li>A black box <strong>LLM API</strong> $LLM(p, x) \approx \underset{y \in \mathcal{L}}{\operatorname{argmax}} P_{LLM(y|p, x)}$ , where $y$ is the most likely text continuation of the concatenated prompt $p$ and query $x$</li><li>A <strong>metric</strong> function $m(p, D_{te})$, which quantitatively measures how good the prompt $p$ is on a test dataset $D_{te}$</li></ol><h1 id=proposed-algorithm-for-apo class="group hover:underline hover:underline-offset-6">Proposed Algorithm for APO <a href=#proposed-algorithm-for-apo class="opacity-0 group-hover:opacity-100">#</a></h1><p>The proposed algorithm, iteratively refines the prompt $p_0$ to produce $\hat{p}$, which is an approximation of the optimal prompt $p^* = \underset{y\in \mathcal{L}}{\operatorname{argmax}} m(p, D_{te})$. It does so by performing &ldquo;textual gradient decent&rdquo; which &ldquo;guides&rdquo; the prompts towards improvement. Then it uses these gradients to beam search the space of coherent natural language</p><p><figure><a href=/assets/img/blog/2023-06-03-automatic-prompt-optimization/apo_algo.jpg><img src=/assets/img/blog/2023-06-03-automatic-prompt-optimization/apo_algo.jpg alt="Illustration of APO using Gradient Descent"></a><figcaption><strong>Illustration of APO using Gradient Descent:</strong></figcaption></figure><br><em><a href=(http://arxiv.org/abs/2305.03495)>R. Pryzant et al. 2023</a> Illustration of APO: The text dialogue tree we use to mirror gradient descent and overcome the discrete optimization barrier. First, a feedback prompt ∆ generates the gradient g from input data (x, y) and starting prompt p0 and prediction ˆ y (left). Second, an editing prompt δ applies the gradient g to the prompt p0 to produce an improved prompt p′ (right).</em></p><p>Since this body of work uses the term &ldquo;gradient descent&rdquo; from a well-known literature in optimization theory, I find that it&rsquo;s imperative <strong>we draw an analogy</strong> of the <em>textual gradient descent</em> to the <em>regular gradient descent</em>.</p><p>In a regular gradient descent, we have a function $f$ parameterized by $\theta$, which given an input $x$ returns an output $y$. Our objective is to find the best parameters $\theta^*$, such that the error in prediction a.k.a loss $J(\theta) = y - \hat{y}$ is minimized. Iteratively, $\theta$ is updated by a small amount $\alpha$ in the opposite direction of the gradient $\frac{\partial J(\theta)}{\partial \theta}$ until convergence.</p><p>$$
f_\theta(x) = \hat{y} \
J(\theta) = y - \hat{y} \
\theta_{t+1} = \theta_t - \alpha \frac{\partial}{\partial \theta} J(\theta)
$$</p><p>In this work, we can treat function $f$ to be the <strong>LLM API</strong> and the output of the <strong>LLM API</strong> to be parameterized by the prompt $p$. The loss term here is the error in prediction $J(\theta) \approx LLM_p(x) - y$. Finally the updated prompt $p_{t+1}$ is the output of updating the current prompt $p_t$ using the gradients $\frac{\partial J(\theta)}{\partial \theta}$.</p><p><strong>Note:</strong> All the operation here are in a discrete &ldquo;coherent natural language space&rdquo; $\mathcal{L}$ instead of the the real number space $\mathbb{R}$. Thus, each operation in the $\mathbb{R}$ space in <em>regular grad-descent</em> must have an analogy in the <em>textual grad-descent</em> process.</p><h2 id=gradient-descent-with-prompts class="group hover:underline hover:underline-offset-6">Gradient Descent with Prompts <a href=#gradient-descent-with-prompts class="opacity-0 group-hover:opacity-100">#</a></h2><p>Gradients provide a direction in which the parameter has to be updated. In this setting, gradient descent refers to</p><ol><li>Evaluating a prompt on some batch of data $D_{te}$ and collect the prediction errors: $\mathcal{E}$</li><li>Creating a local loss signal, which has qualitative information about what can be improved in the prompt to reduce $\mathcal{E}$</li><li>Editing or Updating the prompt $p_t$ in the opposite semantic direction of the gradient.</li></ol><p>This is achieved by using the <strong>LLM API</strong> again with <strong>2 static prompts</strong>.</p><ol><li>$\nabla$ prompt: This prompt creates the local loss signal $g = LLM_{p^{\nabla}}(p_t, \mathcal{E})$ which produced the summary of flaws of $p_t$</li><li>$\delta$ prompt: This prompts generates new $k$ prompts $p_{t+1}$ by editing the previous prompt $p_t$ i.e. $LLM_{p^{\delta}}(p_t, g) \rightarrow {p_{t+1}^0, p_{t+1}^1, \dots, p_{t+1}^k}$</li></ol><p><strong>Examples of the prompts used in paper</strong></p><ul><li>Example of $\nabla$ prompt<pre tabindex=0><code>I&#39;m trying to write a zero-shot classifier prompt. 

My current prompt is: &#34;{prompt}&#34; 

But this prompt gets the following examples wrong: {error_string} 

give {num_feedbacks} reasons why the prompt could have gotten these examples wrong.

Wrap each reason with &lt;START&gt; and &lt;END&gt;
</code></pre></li><li>Example of $\delta$ prompt<pre tabindex=0><code>I&#39;m trying to write a zero-shot classifier. 

My current prompt is: &#34;{prompt}&#34; 

But it gets the following examples wrong: {error_str} 

Based on these examples the problem with this prompt is that {gradient} 

Based on the above information, I wrote {steps_per_gradient} different improved prompts. 

Each prompt is wrapped with &lt;START&gt; and &lt;END&gt;. 

The {steps_per_gradient} new prompts are:
</code></pre></li></ul><h2 id=beam-search-over-prompts class="group hover:underline hover:underline-offset-6">Beam search over prompts <a href=#beam-search-over-prompts class="opacity-0 group-hover:opacity-100">#</a></h2><p>Regular gradient descent provides a <em>guarantee</em> that the parameter update operation $\theta_{t+1} = \theta_t - \alpha \frac{\partial}{\partial \theta} J(\theta)$ reduces the prediction error in the subsequent step. However, in this case since, the gradients are qualitative errors and an LLM is used to generate the updated prompt, we lose the <em>guarantee</em> that the prediction error will be reduced.</p><p>To tackle this loss of <em>guarantee</em>, beam search is used to maintain a <em>beam</em> or <em>pool</em> of prompts and search through $\mathcal{L}$. The beam search consists of two steps:</p><ol><li><strong>Expansion Step</strong>: Sample possible prompts (using an LLM) in the discrete space, and then</li><li><strong>Selection Step</strong>: Prune out the prompts which produce a bad evaluation score on the test dataset.</li></ol><h3 id=expansion-step class="group hover:underline hover:underline-offset-6">Expansion Step <a href=#expansion-step class="opacity-0 group-hover:opacity-100">#</a></h3><p>The expansion step is used to generate many new candidate prompts from existing prompts, ideally better prompts directed by the gradients.</p><p><figure><a href=/assets/img/blog/2023-06-03-automatic-prompt-optimization/expansion_step.jpg><img src=/assets/img/blog/2023-06-03-automatic-prompt-optimization/expansion_step.jpg alt="Expansion Step"></a><figcaption><strong>Expansion Step:</strong></figcaption></figure><em>The algorithm for the expansion step</em></p><p><strong>Note:</strong> In Step 5, monte carlo successors refers to prompts which are differently worded but semantically similar to their inputs. (My personal take: I&rsquo;m not sure why they call this kind of expansion the local monte-carlo search space)</p><h3 id=selection-step class="group hover:underline hover:underline-offset-6">Selection Step <a href=#selection-step class="opacity-0 group-hover:opacity-100">#</a></h3><p>After the new prompts are generated, they are evaluated on the test set, and the weaker prompts are pruned out till the beam width is reached again.</p><p>However, naively evaluating each prompt on the entire dataset is expensive <a href=https://arxiv.org/pdf/2203.07281.pdf>Prasad et al., 2022</a>. When under as evaluation budget, the selection step must be computed efficiently.</p><p><figure><a href=/assets/img/blog/2023-06-03-automatic-prompt-optimization/selection_step.jpg><img src=/assets/img/blog/2023-06-03-automatic-prompt-optimization/selection_step.jpg alt="Selection Step"></a><figcaption><strong>Selection Step:</strong></figcaption></figure><br><em>Selection Steps: (1) Select using UCG (2) Selection using Successive Rejects.</em></p><p>The authors propose two algorithms:</p><ol><li>The UCB: This chooses the best arm in the multi-arm bandit formulations, where $b$ best arms are to be selected with the least &ldquo;pulls&rdquo; as possible.</li><li>Successive Rejects: This maintains a list of surviving prompts, where one prompt is rejected at each iteration. It is done by using all prompts on $n_t$ random points and then dropping the prompt which ges the lowest score. Unlike UCB this requires no hyperparameters.</li></ol><h2 id=experiments class="group hover:underline hover:underline-offset-6">Experiments <a href=#experiments class="opacity-0 group-hover:opacity-100">#</a></h2><p>I would leave it to the reader to go through the experiments from the paper itself.</p><h1 id=conclusions class="group hover:underline hover:underline-offset-6">Conclusions <a href=#conclusions class="opacity-0 group-hover:opacity-100">#</a></h1><p>This work is one of the many works which try to optimizize a prompt for a task. Other works include (1) Automatic Prompt Engineer <a href=http://arxiv.org/abs/2211.01910>Zhou et al.</a> (2) GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models (<a href=http://arxiv.org/abs/2203.07281>Prasasd et al.</a>)</p><p>I like the authors approach to this work as they develop a method around the well established &ldquo;gradient descent&rdquo; and beam search concept.<br>However, I would have liked if the paper:</p><ol><li>Delved deep into why can LLMs critique themselves but not perform the desierd task?</li><li>If LLMs can critique themselves, why is an iterative approach required? Why can&rsquo;t LLMs give the best prompt in one go?</li><li>Finally, can this approach lead to over fitting to the $D_{tr}$? i.e, if the prompt is optimizited for a Task which is represented by $D_{tr}$ and optimized based on performance on $D_{te}$, can it generalize to tasks in $T - {D_{tr} \cup D_{te}}$</li></ol><hr><h2 id=citation class="group hover:underline hover:underline-offset-6">Citation <a href=#citation class="opacity-0 group-hover:opacity-100">#</a></h2><p>Cited as</p><pre tabindex=0><code>@article{pyne2023apo,
  title   = &#34;Summary of Automatic Prompt Optimization&#34;,
  author  = &#34;Pyne, Aishik&#34;,
  journal = &#34;aishikpyne.com&#34;,
  year    = &#34;2023&#34;,
  month   = &#34;June&#34;,
  url     = &#34;https://aishikpyne.com/blog/2023-06-03-automatic-prompt-optimization/&#34;
}
</code></pre><h2 id=references class="group hover:underline hover:underline-offset-6">References <a href=#references class="opacity-0 group-hover:opacity-100">#</a></h2><p>[1] R. Pryzant, D. Iter, J. Li, Y. T. Lee, C. Zhu, and M. Zeng, <a href=http://arxiv.org/abs/2305.03495>“Automatic Prompt Optimization with ‘Gradient Descent’ and Beam Search.”</a> arXiv, May 04, 2023. doi: 10.48550/arXiv.2305.03495.</p><p>[2] A. Prasad, P. Hase, X. Zhou, and M. Bansal, <a href=https://arxiv.org/pdf/2203.07281.pdf>“GrIPS: Gradient-free, Edit-based Instruction Search for Prompting Large Language Models.”</a> arXiv, Apr. 26, 2023. doi: 10.48550/arXiv.2203.07281.</p></article></div><footer id=footer class="relative max-w-7xl mx-auto w-full
px-4 tablet:px-8
mt-8 tablet:mt-12 desktop:mt-16"><div class="relative mx-auto
max-w-2xl tablet:max-w-4xl desktop:max-w-5xl
tablet:px-4 desktop:px-8"><div class="relative tablet:px-4 desktop:px-6"><hr class="my-4 border-zinc-200 sm:mx-auto dark:border-zinc-700 tablet:my-3"><div class=mx-auto><div class="flex flex-col w-full gap-4 py-4"><a href=https://aishikpyne.com/ class="mb-4 tablet:mb-0"><span class="text-center whitespace-nowrap font-signature text-lg font-semibold dark:text-white">Aishik
Pyne</span></a><nav class="flex flex-col items-start justify-between gap-6 tablet:flex-row"><div class="flex flex-col flex-wrap justify-start tablet:justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200 tablet:flex-row"><a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/about>About</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/projects>Projects</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/articles>Articles</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/photography>Photography</a></div><p class="text-sm text-zinc-400 dark:text-zinc-500">© 2023 Aishik Pyne. All
rights reserved.</p></nav></div></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/flowbite/1.6.6/flowbite.min.js></script>
<script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?themeToggleLightIcon.classList.remove("hidden"):themeToggleDarkIcon.classList.remove("hidden"),themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))})</script></body></html>