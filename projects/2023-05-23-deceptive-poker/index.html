<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.1.1/css/all.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/brands.min.css><link rel=stylesheet type=text/css href=https://aishikpyne.com/css/style.css><script src=https://aishikpyne.com/js/darkmode.js></script><title>Aishik Pyne</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-GKMW1CE3H9"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-GKMW1CE3H9")</script><script type=text/x-mathjax-config>
        MathJax.Hub.Config({
          TeX: {
              equationNumbers: {
                  autoNumber: "AMS"
              }
          },
          tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
              processEscapes: true,
          }
        });
        MathJax.Hub.Register.MessageHook("Math Processing Error", function (message) {
            alert("Math Processing Error: " + message[1]);
        });
        MathJax.Hub.Register.MessageHook("TeX Jax - parse error", function (message) {
            alert("Math Processing Error: " + message[1]);
      });
      </script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"></script></head><body class="bg-zinc-50 dark:bg-black h-screen flex flex-col"><div class="fixed inset-0 flex justify-center tablet:px-8"><div class="flex w-full max-w-7xl desktop:px-8"><div class="w-full bg-white ring-1 ring-zinc-100 dark:bg-zinc-900 dark:ring-zinc-300/20"></div></div></div><nav class="fixed w-full mx-auto z-20 top-0 tablet:px-8"><div class="mx-auto max-w-7xl w-full desktop:px-8 mb-10"><div class="flex flex-wrap items-center p-4 tablet:px-4 bg-white dark:bg-zinc-900 gap-4"><div class="flex initial order-first"><a href=https://aishikpyne.com/ class="flex items-center"><span class="self-center whitespace-nowrap font-signature text-lg font-semibold dark:text-white">Aishik Pyne</span></a></div><div id=navbar-search class="hidden tablet:block mx-auto justify-center tablet:order-2 order-4 grow"><div class="flex justify-center"><ul class="flex items-center tablet:w-auto w-min justify-center tablet:flex-wrap rounded-full bg-white/90 px-3 text-sm font-medium text-zinc-800 shadow-lg shadow-zinc-800/5 ring-1 ring-zinc-900/5 backdrop-blur dark:bg-zinc-800/90 dark:text-zinc-200 dark:ring-white/10"><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/about>About</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/projects>Projects</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/articles>Articles</a></li><li><a class="relative block px-3 py-2 transition hover:text-pink-500 dark:hover:text-pink-400" href=/photography>Photography</a></li></ul></div></div><div class="flex grow tablet:grow-0 items-end justify-end order-3"><button id=theme-toggle type=button class="mx-2 rounded-lg p-2.5 text-sm text-zinc-500 hover:bg-zinc-100 focus:outline-none focus:ring-4 focus:ring-zinc-200 dark:text-zinc-400 dark:hover:bg-zinc-700 dark:focus:ring-zinc-700"><svg id="theme-toggle-dark-icon" class="hidden h-5 w-5" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M17.293 13.293A8 8 0 016.707 2.707a8.001 8.001.0 1010.586 10.586z"/></svg><svg id="theme-toggle-light-icon" class="hidden h-5 w-5" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path d="M10 2a1 1 0 011 1v1A1 1 0 119 4V3a1 1 0 011-1zm4 8a4 4 0 11-8 0 4 4 0 018 0zm-.464 4.95.707.707a1 1 0 001.414-1.414l-.707-.707a1 1 0 00-1.414 1.414zm2.12-10.607a1 1 0 010 1.414l-.706.707A1 1 0 1113.536 5.05l.707-.707a1 1 0 011.414.0zM17 11a1 1 0 100-2h-1a1 1 0 100 2h1zm-7 4a1 1 0 011 1v1a1 1 0 11-2 0v-1a1 1 0 011-1zM5.05 6.464A1 1 0 106.465 5.05l-.708-.707A1 1 0 004.343 5.757l.707.707zm1.414 8.486-.707.707a1 1 0 01-1.414-1.414l.707-.707a1 1 0 011.414 1.414zM4 11a1 1 0 100-2H3a1 1 0 000 2h1z" fill-rule="evenodd" clip-rule="evenodd"/></svg></button>
<button data-collapse-toggle=navbar-search type=button class="inline-flex items-center rounded-lg p-2 text-sm text-gray-500 hover:bg-gray-100 focus:outline-none focus:ring-2 focus:ring-gray-200 dark:text-gray-400 dark:hover:bg-gray-700 dark:focus:ring-gray-600 tablet:hidden" aria-controls=navbar-search aria-expanded=false>
<span class=sr-only>Open menu</span><svg class="h-6 w-6" aria-hidden="true" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M3 5a1 1 0 011-1h12a1 1 0 110 2H4A1 1 0 013 5zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1zm0 5a1 1 0 011-1h12a1 1 0 110 2H4a1 1 0 01-1-1z" clip-rule="evenodd"/></svg></button></div></div></div></nav><div class="relative flex flex-col w-full mx-auto pt-16"><div class="max-w-[720px] m-6 mx-auto px-4 tablet:px-8"><header class="not-format mb-4 lg:mb-6"><h1 class="mb-2 text-3xl font-extrabold leading-tight text-gray-900 dark:text-white tablet:text-3xl">Cheating Detection in Competitive Multi-Agent Environments</h1><p class="text-sm font-light text-gray-500 dark:text-gray-400">May 6, 2023 · Aishik Pyne · 14 min</p></header><div class="mx-auto mb-4 max-w-prose rounded-lg border border-zinc-200 bg-white px-4 py-2 shadow dark:border-zinc-700 dark:bg-zinc-800 dark:text-white" id=accordion-collapse data-accordion=collapse><div id=accordion-collapse-heading-1 class=m0><button type=button class="flex w-full cursor-zoom-in items-center justify-between bg-inherit text-left font-medium text-zinc-900 dark:bg-inherit dark:text-zinc-200" data-accordion-target=#accordion-collapse-body-1 aria-expanded=false aria-controls=accordion-collapse-body-1>
<span class=text-bold>Table of Content</span><svg class="h-6 w-6 shrink-0" fill="currentcolor" viewBox="0 0 20 20" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" d="M5.293 7.293a1 1 0 011.414.0L10 10.586l3.293-3.293a1 1 0 111.414 1.414l-4 4a1 1 0 01-1.414.0l-4-4a1 1 0 010-1.414z" clip-rule="evenodd"/></svg></button></div><div id=accordion-collapse-body-1 class="prose prose-sm prose-a:no-underline hidden leading-tight dark:prose-invert" aria-labelledby=accordion-collapse-heading-1><nav id=TableOfContents><ul><li><a href=#abstract>Abstract</a></li><li><a href=#introduction>Introduction</a></li><li><a href=#related-work>Related Work</a></li><li><a href=#background>Background</a><ul><li><a href=#poker-rules>Poker Rules</a></li><li><a href=#pomdp>POMDP</a></li></ul></li><li><a href=#methodology>Methodology</a><ul><li><a href=#competitive-agents>Competitive Agents</a></li><li><a href=#collaborative-agents>Collaborative Agents</a></li><li><a href=#discriminator>Discriminator</a></li></ul></li><li><a href=#experiments>Experiments</a><ul><li><a href=#training-results-for-competitive-agent>Training Results for Competitive agent</a></li><li><a href=#training-results-for-multi-agent>Training Results for Multi agent</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#future-works--discussions>Future Works & Discussions</a></li><li><a href=#resources>Resources</a></li><li><a href=#citation>Citation</a></li><li><a href=#references>References</a></li></ul></nav></div></div><article class="mx-auto prose prose-zinc dark:prose-invert
prose-base max-w-full leading-relaxed
prose-img:mx-auto prose-img:rounded-lg
prose-figcaption:-mt-6 prose-figcaption:text-center"><p>Poker is supposed to competitively, each for their own. However, if two players team up and decide to cheat by collaborating, can an AI system, analyze play patterns and detect which players are cheating and which players are playing fairly?</p><h2 id=abstract class="group hover:underline hover:underline-offset-6">Abstract <a href=#abstract class="opacity-0 group-hover:opacity-100">#</a></h2><p>Multi-agent reinforcement learning (MARL) is a prevalent learning paradigm
for solving stochastic games. Specifically, imperfect information games model
strategic interactions between multiple agents with only partial information which
can be modelled as a POMDP [3].
In most MARL studies, agents in a game are defined as teammates or enemies beforehand and these relationships among the agents are known to all. However, in real-world problems, the agent relationships are commonly unknown in advance. Therefore, training policies for such situations in the face of imperfect information and ambiguous identities is a significant problem that needs to be addressed.</p><p>In this project, we aim to model the scenario for multi-agent partially-observable zero-sum games (e.g. poker, red-10) where M agents play collaboratively against $(N −M)$ competitive agents.
However, the $(N−M)$ agents unknowingly assume a fully competitive setting.
With such a secret arrangement between the collaborative agents to split the reward equally after a game, we wish to observe if the collaborative agents can develop a strategy to beat the competitive agents on expectation.
We also provide a discriminator that observes each agent’s entire gameplay and detects cheating (a collaboration between $M$ agents)</p><h2 id=introduction class="group hover:underline hover:underline-offset-6">Introduction <a href=#introduction class="opacity-0 group-hover:opacity-100">#</a></h2><p>Most AI systems nowadays are based on a single agent tackling a task, or in the case of adversarial models, multiple agents compete against each other to improve the overall behaviour of a system \cite{marl:blog}. However, many real-world scenarios require a mixed setting where multiple agents act either collaboratively or competitively with each other at different timesteps to get the best returns for themselves (e.g., driving a car to reach the destination in the shortest time while avoiding traffic). In this aspect, multi-agent reinforcement learning (MARL) is a prevalent learning paradigm for solving stochastic games. Imperfect-information games model strategic interactions between multiple agents with only partial information (e.g., poker, blackjack, red-10). Typically in such games, one seeks a Nash equilibrium \cite{NashEq}, in which no player can improve by deviating from the equilibrium. Currently, the most successful algorithms for partially observable zero-sum games are Deep CFR \cite{dcfr:2018}, NFSP \cite{nsfp:2016}, and ReBel \cite{rebel:2020}.</p><p>It is interesting to note that in most multi-agent works, whether an agent is acting collaboratively or competitively with other agents is known by everyone from the beginning which affects everyone&rsquo;s policies. However, in a real-life scenario, such an assumption may not always hold (e.g., a game of poker where 2 agents have teamed up against all other players with the intent of cheating). Therefore, we aim to model such a partially-observable mixed environment where individual agents attempt to maximize their reward assuming a fully competitive setting, but the teammates acting collaboratively are expected to win most games on average by maximizing their combined reward, at the expense of lower individual rewards. Such cheating scenarios are common in highly-dynamic, complex environments such as casinos. Therefore, we also train a cheating detector system that is able to observe the entire game state at all times along with the game-plays of every agent and is able to classify if there was any foul play during the game. Our contributions are as follows:</p><ul><li><p>We train poker agents where the policy for each agent is learned in a fully-competitive setting using NFSP \cite{nsfp:2016}.</p></li><li><p>We train poker agents where two agents play collaboratively against $N$ competitive players in order to beat them in most games. Just as in a real-life case, each player&rsquo;s game space is still partially observable but the expected team reward is maximized.</p></li><li><p>Model a discriminator that learns to distinguish between these two styles of play (competitive vs collaborative) by comparing the actions taken by each player with our trained policies.</p></li></ul><h2 id=related-work class="group hover:underline hover:underline-offset-6">Related Work <a href=#related-work class="opacity-0 group-hover:opacity-100">#</a></h2><p>There are many approaches developed for finding nash equilibrium in games with imperfect information \cite{rebel:2020, dcfr:2018, nsfp:2016, deepstack}. We will be focusing on the two most popular approaches, Neural Fictitious Self Play and Deep Counterfactual Regret Minimization.</p><p>Neural Fictitious Self Play (NFSP) \cite{nsfp:2016} is one of the algorithms used to train agents for imperfect information games (eg. poker) which combines fictitious self play with neural networks. It is a two stage process, which can be thought of as an actor-critic framework. There is a critic which is trained using DQN. Critic stores all the $&lt;S, A, R, S&rsquo;>$ tuples in it&rsquo;s buffer and it is used to predict the value of the actions taken by the agent. The second part is an actor, which is trained using supervised learning. It&rsquo;s buffer is populated with $&lt;S, A>$ pairs only when we sample the best response action from the critic. It learns the average policy and is used during evaluation.</p><p>% During training, the agent plays against itself. The algorithm consists of two memory buffers, first contains the experience collected from playing other agents. This memory is used to predict the value of the actions taken by the agent. Second, contains the experience of agent&rsquo;s own behaviour which is used to keep track of its own average behavior.</p><p>Deep Counterfactual Regret Minimization (Deep CFR) \cite{dcfr:2018}, it combines CFR with neural networks. It builds on CFR which traverses the search space and accumulates the regrets at each stage. Strategy is chosen based on algorithms which work on the principle of regret minimization. CFR cannot be used to for games with large state space, to solve this Deep CFR uses neural networks to approximate the rest of the state space. In Deep CFR at every iteration the game tree is partially traversed $K$ times and the paths are sampled using MCCFR. After the traversals, a neural network is used to predict the value of exploring that node. This can be thought of as instantaneous regrets in the CFR algorithm.</p><p>The above methods consider only one to one competitive play, for problems with mixed environment having $N \geq 3$ players the most popular algorithm is MADDPG \cite{maddpg}. MADDPG works by having a centralized critic that is trained using global observation and the private observation of the agents, but at test time the agents only act on the local information.</p><h2 id=background class="group hover:underline hover:underline-offset-6">Background <a href=#background class="opacity-0 group-hover:opacity-100">#</a></h2><h3 id=poker-rules class="group hover:underline hover:underline-offset-6">Poker Rules <a href=#poker-rules class="opacity-0 group-hover:opacity-100">#</a></h3><p>Limit Texas Hold&rsquo;em Poker consists of $N$ players playing for four rounds. Each player get two hand cards from a deck of $52$ shuffled cards, and at each round new community cards are revealed. In each round a player can choose between four actions (call, check, raise, fold). In Limit Texas Hold&rsquo;em the number of raises is capped to four in each rounds, and the max raise amount is $1.5$x the big blind. After four rounds, the best hand wins and that player gets the total chips in the pot. Since it is a zero sum game, the sum of rewards of all the players should sum to zero. This means that there will be players with negative rewards.</p><h3 id=pomdp class="group hover:underline hover:underline-offset-6">POMDP <a href=#pomdp class="opacity-0 group-hover:opacity-100">#</a></h3><p>We model Poker as a Partially Observable Markov Decision Process (POMDP) which is described as the tuple $&lt; S, A, T, R, O>$ where:</p><ul><li>$S$ - finite set of states of the environment (game space). For Limit Texas Hold&rsquo;em Poker, the game space is of the order $10^14$.</li><li>$A$ - finite set of actions each agent can take at each turn (Raise, Fold, Call, Check).</li><li>$T: S$ x $A \rightarrow \Delta(S)$ - state-transition function giving a distribution over states of the environment, given a starting state and an action performed by the agent.</li><li>$R$ - the reward function, giving a real-values expected immediate reward, given a starting state and an action performed by the agent. (money/chips after every game).</li><li>$\Omega$ - finite set of observations the agent can experience.</li><li>$O: S \times A \rightarrow \Delta(\Omega)$ - the observation function, giving a distribution over possible observations, given a starting state and an action performed by the agent.</li></ul><h2 id=methodology class="group hover:underline hover:underline-offset-6">Methodology <a href=#methodology class="opacity-0 group-hover:opacity-100">#</a></h2><p>The section is divided into three parts. In \ref{Competitive Agent} we show how we encode the state space, evaluate and train our agents, and in \ref{Collaborative Agent} we discuss how we model the communication. Finally, \ref{Discriminator} elaborates on how we take advantage of the tractable action space to discriminate between two agents.</p><h3 id=competitive-agents class="group hover:underline hover:underline-offset-6">Competitive Agents <a href=#competitive-agents class="opacity-0 group-hover:opacity-100">#</a></h3><p>We encode the space space of limit hold&rsquo;em poker into seventy two bits. The first fifty two bits contain the hand cards and the community cards. The last twenty bits contains the number of raises in each round. Limit hold&rsquo;em has four rounds and each round can have a maximum of four raises. Therefore, we one hot encode this information into twenty bits. This allows us to make the state space independent of the number of players, and also includes the information about the actions taking place in the game. This way of encoding the state space is taken from the RLCard \cite{rlcard} library.</p><p>For our setting we consider a three player setting. In this setting all the agents are playing to maximize their own reward i.e. the chips they win at the end of the round. We initialize a policy network which we will refer to as &ldquo;Brain 1&rdquo;. We first simulate a game with all the three players and generate the trajectories. Then we feed this trajectories to the neural network and train Brain 1 using the NFSP \cite{nsfp:2016} algorithm.</p><h3 id=collaborative-agents class="group hover:underline hover:underline-offset-6">Collaborative Agents <a href=#collaborative-agents class="opacity-0 group-hover:opacity-100">#</a></h3><p>This setting contains three agents, out of which two agents are collaborating to maximize their collective reward. To model the communication between two agents we explore two scenarios. First, the agents simply know if their friend has raised in the round. This simulates real life setting where collaborating agents need to infer from the actions. Second, we give the hand cards of the other agent along with the previous information.<br>In this setting we set up rewards in the following ways.</p><p>$$
\mathcal{R}(s, a)=\begin{cases}
\max{c_1, c_2}, & c_3 \leq 0\
\displaystyle \frac{c_1 + c_2}{2}, & c_3 > 0\
\end{cases}
$$
where $c_{i}$ is the number of chips won by player $i$ after action $a$ performed in state $s$.</p><p>Using the above reward function we train two agents using a policy which we will call &ldquo;Brain 2&rdquo;. During training we are freezing the weights of the competitive agent (Brain 1) and initializing the collaborative agent with the weights of Brain 1.</p><h3 id=discriminator class="group hover:underline hover:underline-offset-6">Discriminator <a href=#discriminator class="opacity-0 group-hover:opacity-100">#</a></h3><p>The action space of the game is extremely small compared to the state space ($4$ vs $10^14$). Therefore, we take advantage of the small action space and create a discriminator that doesn&rsquo;t need to enumerate the state space to detect cheating. The discriminator analyses the entire trajectory of an agent and predicts if the behaviour is closely related to Brain1 or Brain2. In Figure \ref{fig:cheating}, we see that only enumerating the actions that match with either of the Brains and not both (green and blue boxes) is sufficient to determine the behaviour of the agent on expectation over multiple games.</p><p><figure><a href=./images/discriminator.png title="The pink boxes show no information gain since both the Brains exhibit the same behaviour. The green and blue boxes depict the actions of each agent that match Brain1 and Brain2 respectively. "><img src=./images/discriminator.png alt=Discriminator></a><figcaption><strong>Discriminator:</strong>
The pink boxes show no information gain since both the Brains exhibit the same behaviour. The green and blue boxes depict the actions of each agent that match Brain1 and Brain2 respectively.</figcaption></figure></p><p>Specifically, there is no information gain in terms of behaviour deduction if both brains choose the same action for a given state (pink boxes). However, for state spaces that provide different actions for both brains (green and blue boxes), we sum the number of times the agent matches the actions with a specific brain. Over a large number of games, the agent&rsquo;s true behaviour would closely imitate the behaviour of either brain. Therefore, the discriminator would be able to detect if an agent is cheating or playing fairly over an expectation.</p><h2 id=experiments class="group hover:underline hover:underline-offset-6">Experiments <a href=#experiments class="opacity-0 group-hover:opacity-100">#</a></h2><p>This section will discuss the parameters and the results of the following experiments.</p><h3 id=training-results-for-competitive-agent class="group hover:underline hover:underline-offset-6">Training Results for Competitive agent <a href=#training-results-for-competitive-agent class="opacity-0 group-hover:opacity-100">#</a></h3><p>We trained the three agents for $85k$ games and evaluated the agents against rule based poker agent and random agent. Brain 1 (policy network) consisted of a simple two layer mlp with $64$ neurons and was trained using the nfsp \cite{nsfp:2016} algorithm.<figure><a href=./images/single_rule.png><img src=./images/single_rule.png alt="&amp;ldquo;Figure: Brain 1 vs Rule Based agents accumulated reward over number of games.&amp;rdquo;"></a><figcaption><strong>&ldquo;Figure: Brain 1 vs Rule Based agents accumulated reward over number of games.&rdquo;:</strong></figcaption></figure></p><p>We compare Brain 1 with a rule based poker agent, which takes actions by calculating the probability of winning based on the hand and community cards. We do this to see if our policy is making complex strategies which do not rely on simple probabilities. Brain 1 achieves a win ratio of $3.66$ over the rule based agent, and we can see that in figure \ref{fig:single_rule} Brain 1 is accumulating more rewards over time.</p><p>Since our framework does not depend on the type of algorithm we use to train, we can confidently say that Brain 1 has at least learned some complex strategy to defeat naive poker algorithms.</p><h3 id=training-results-for-multi-agent class="group hover:underline hover:underline-offset-6">Training Results for Multi agent <a href=#training-results-for-multi-agent class="opacity-0 group-hover:opacity-100">#</a></h3><p>In this setting we kept the policy network size same, to maintain consistency. We trained the collaborative agents for $85k$ games and evaluated based on the win ratio and rewards accumulated over time.</p><p>As we can see in Figure \ref{fig:train_4_bits_multi} that collaborative agents over time learn how to consistently defeat the competitive agent. This means that just by adding the information if the collaborative agent raised in that round, is enough to make it outperform. We also simulate $100k$ independent games and find that collaborative agents have a win ratio of $1.14$ over the competitive agent. Which means that on average it is winning $14%$ more games.</p><p><figure><a href=./images/train_4bits_multi.png><img src=./images/train_4bits_multi.png alt="Figure: Training graph of collaborative setting"></a><figcaption><strong>Figure: Training graph of collaborative setting:</strong></figcaption></figure></p><p>We find out the marginal action distribution of Brain 1 vs Brain 2. Since state space is intractable, we are computing trajectories for $100k$ independent games and keeping track of the actions taken by the policies. In figure \ref{fig:confusion_matrix} we can see that Brain 2 is performing more number of raises. Which makes intuitive sense, since collaborative agents have more chances of winning, it can afford to take more risks.</p><p><figure><a href=./images/bar_graph.png><img src=./images/bar_graph.png alt="Figure: Action distribution of Brain 1 vs Brain 2, $p(B_1, B_2) (Cumulative)$"></a><figcaption><strong>Figure: Action distribution of Brain 1 vs Brain 2, $p(B_1, B_2) (Cumulative)$:</strong></figcaption></figure><figure><a href=./images/confusion_matrix.png><img src=./images/confusion_matrix.png alt="Figure: Action distribution of Brain 1 vs Brain 2, $p(B_1, B_2)$"></a><figcaption><strong>Figure: Action distribution of Brain 1 vs Brain 2, $p(B_1, B_2)$:</strong></figcaption></figure></p><table><thead><tr><th style=text-align:center>Rule Based Agent</th><th style=text-align:center>Cheater 1</th><th style=text-align:center>Cheater 2</th></tr></thead><tbody><tr><td style=text-align:center><figure><a href=./images/rulebased.png><img src=./images/rulebased.png alt="&amp;ldquo;Rule Based Agent&amp;rdquo;"></a><figcaption><strong>&ldquo;Rule Based Agent&rdquo;:</strong></figcaption></figure></td><td style=text-align:center><figure><a href=./images/cheater1.png><img src=./images/cheater1.png alt="&amp;ldquo;Cheater 1&amp;rdquo;"></a><figcaption><strong>&ldquo;Cheater 1&rdquo;:</strong></figcaption></figure></td><td style=text-align:center><figure><a href=./images/cheater2.png><img src=./images/cheater2.png alt="&amp;ldquo;Cheater 2&amp;rdquo;"></a><figcaption><strong>&ldquo;Cheater 2&rdquo;:</strong></figcaption></figure></td></tr></tbody></table><p>The plots in Figure \ref{fig:discriminator} show that a rule-based agent is similar to a competitive (fair) player, whereas both the collaborative agents demonstrate high foul play behaviours over an increasing number of games. This is an intuitive result since a rule-based agent has no information about any other player&rsquo;s hand or the motivation for making any raises (bets), and plays only to maximize their own reward.</p><h2 id=conclusion class="group hover:underline hover:underline-offset-6">Conclusion <a href=#conclusion class="opacity-0 group-hover:opacity-100">#</a></h2><p>In this project, we have successfully trained multi-agents in a fully competitive and mixed environment to play a zero-sum game. We also modelled a strategy that enables the collaborative agents to win over the competitive agents on average and analysed this performance using multiple evaluation metrics. Finally, we provided an easy method to model a discriminator that can classify the behaviour of the agents as either fair or foul by analysing their individual gameplays.</p><h2 id=future-works--discussions class="group hover:underline hover:underline-offset-6">Future Works & Discussions <a href=#future-works--discussions class="opacity-0 group-hover:opacity-100">#</a></h2><p>Current evaluation and training methods are done on independent games, which makes the problem easier. So there is no long term strategy employed by the agents to win the game. We plan to train the agents on dependent games, where the winning of one game will impact the next game. Moreover, it is not intuitive that just by adding the information of which player raised makes the collaborative agent outperform the competitive agent. While there is strict information gain, it doesn&rsquo;t lead to any semantic meaning in real life setting.</p><h2 id=resources class="group hover:underline hover:underline-offset-6">Resources <a href=#resources class="opacity-0 group-hover:opacity-100">#</a></h2><ul><li>Download the PPT: <a href=./documents/deceptive_poker_ppt.pdf>DeceptivePokerPresentaion</a></li><li>Download Project Report in PDF: <a href=./documents/deceptive_poker_report.pdf>DeceptivePokerReport</a></li></ul><h2 id=citation class="group hover:underline hover:underline-offset-6">Citation <a href=#citation class="opacity-0 group-hover:opacity-100">#</a></h2><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-text data-lang=text><span style=display:flex><span>@article{pyne2023deceptivepoker,
</span></span><span style=display:flex><span>  title   = &#34;Deceptive Poker: Cheating Detection in Competitive Multi-Agent Environments&#34;,
</span></span><span style=display:flex><span>  author  = &#34;Pyne, Aishik, Abhichandani Harshavarshan, Shrivastava Niharika&#34;
</span></span><span style=display:flex><span>  journal = &#34;aishikpyne.com&#34;,
</span></span><span style=display:flex><span>  year    = &#34;2023&#34;,
</span></span><span style=display:flex><span>  month   = &#34;Jun&#34;,
</span></span><span style=display:flex><span>  url     = &#34;https://aishikpyne.com/projects/2023-05-23-deceptive-poker/&#34;
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><h2 id=references class="group hover:underline hover:underline-offset-6">References <a href=#references class="opacity-0 group-hover:opacity-100">#</a></h2><p>[1] Bakhtin A. Lerer A. Gong Q. Brown, N. Combining deep reinforcement learning and search for
imperfect-information games. ArXiv. /abs/2007.13544, 2020.</p><p>[2] Lerer A. Gross S. Sandholm T. Brown, N. Deep counterfactual regret minimization. ArXiv.
/abs/1811.00164, 2018.</p><p>[3] Michael Hahsler and Hossein Kamalzadeh. <a href=%22https://cran.r-project.org/web/packages/pomdp/vignettes/POMDP.html%22>Pomdp: Introduction to partially observable markov
decision processes.</a>,
2021.</p><p>[4] Silver D. Heinrich, J. Deep reinforcement learning from self-play in imperfect-information
games. ArXiv. /abs/1603.01121, 2016.</p><p>[5] Matej Moravˇc ík, Martin Schmid, Neil Burch, Viliam Lisý, Dustin Morrill, Nolan Bard, Trevor
Davis, Kevin Waugh, Michael Johanson, and Michael Bowling. DeepStack: Expert-level artificial
intelligence in heads-up no-limit poker. Science, 356(6337):508–513, may 2017.</p><p>[6] Martin J. Osborne and Ariel Rubinstein. A course in game theory. 1994.</p><p>[7] Jesus Rodriguez. How modern game theory is influencing multi-agent reinforcement learning
systems. Medium: Dataseries, 2020.</p><p>[8] Lai K. Cao Y. Huang S. Wei R. Guo J. Hu X. Zha, D. Rlcard: A toolkit for reinforcement
learning in card games. ArXiv. /abs/1910.04376, 2019.</p></article></div><footer id=footer class="relative max-w-7xl mx-auto w-full
px-4 tablet:px-8
mt-8 tablet:mt-12 desktop:mt-16"><div class="relative mx-auto
max-w-2xl tablet:max-w-4xl desktop:max-w-5xl
tablet:px-4 desktop:px-8"><div class="relative tablet:px-4 desktop:px-6"><hr class="my-4 border-zinc-200 sm:mx-auto dark:border-zinc-700 tablet:my-3"><div class=mx-auto><div class="flex flex-col w-full gap-4 py-4"><a href=https://aishikpyne.com/ class="mb-4 tablet:mb-0"><span class="text-center whitespace-nowrap font-signature text-lg font-semibold dark:text-white">Aishik
Pyne</span></a><nav class="flex flex-col items-start justify-between gap-6 tablet:flex-row"><div class="flex flex-col flex-wrap justify-start tablet:justify-center gap-x-6 gap-y-1 text-sm font-medium text-zinc-800 dark:text-zinc-200 tablet:flex-row"><a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/about>About</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/projects>Projects</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/articles>Articles</a>
<a class="transition hover:text-pink-500 dark:hover:text-pink-400" href=/photography>Photography</a></div><p class="text-sm text-zinc-400 dark:text-zinc-500">© 2023 Aishik Pyne. All
rights reserved.</p></nav></div></div></div></div></footer></div><script src=https://cdnjs.cloudflare.com/ajax/libs/flowbite/1.6.6/flowbite.min.js></script>
<script>var themeToggleDarkIcon=document.getElementById("theme-toggle-dark-icon"),themeToggleBtn,themeToggleLightIcon=document.getElementById("theme-toggle-light-icon");localStorage.getItem("color-theme")==="dark"||!("color-theme"in localStorage)&&window.matchMedia("(prefers-color-scheme: dark)").matches?themeToggleLightIcon.classList.remove("hidden"):themeToggleDarkIcon.classList.remove("hidden"),themeToggleBtn=document.getElementById("theme-toggle"),themeToggleBtn.addEventListener("click",function(){themeToggleDarkIcon.classList.toggle("hidden"),themeToggleLightIcon.classList.toggle("hidden"),localStorage.getItem("color-theme")?localStorage.getItem("color-theme")==="light"?(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark")):(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):document.documentElement.classList.contains("dark")?(document.documentElement.classList.remove("dark"),localStorage.setItem("color-theme","light")):(document.documentElement.classList.add("dark"),localStorage.setItem("color-theme","dark"))})</script></body></html>